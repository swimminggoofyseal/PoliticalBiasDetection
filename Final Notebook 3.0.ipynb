{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Bias In News\n",
    "\n",
    "Data Source: https://www.kaggle.com/snapcrack/all-the-news\n",
    "\n",
    "Leaning Classification: https://www.adfontesmedia.com/Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['/Users/carolynwang/Desktop/PathMentors/Mayuka/Newspapers/articles1.csv', \n",
    "        '/Users/carolynwang/Desktop/PathMentors/Mayuka/Newspapers/articles2.csv',\n",
    "        '/Users/carolynwang/Desktop/PathMentors/Mayuka/Newspapers/articles3.csv']\n",
    "df = pd.concat([pd.read_csv(f) for f in files], ignore_index = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9447ae629b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in text, returns text with stopwords filtered out\n",
    "def stopword_filter(content):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_sentence = []\n",
    "    if isinstance(content, list):\n",
    "        for w in content:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in text, character needed, and boolean (ignore casing or not)\n",
    "#returns number of characters found in text \n",
    "def regex_finder(content, character, ignore_case):\n",
    "    if (ignore_case):\n",
    "        return len(re.findall(character, content, re.I))\n",
    "    else:\n",
    "        return len(re.findall(character, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in pos\n",
    "#takes in text that has been POS tagged\n",
    "#returns frequency of appearance of that particular pos (ex: 0.09 for adjectives)\n",
    "def pos_proportion(content, pos):\n",
    "    pos_list = []\n",
    "    total = [] #all the words\n",
    "    if isinstance(content, list):\n",
    "        for a,b in content:\n",
    "            if b == pos:\n",
    "                pos_list.append(a)\n",
    "            total.append(a)\n",
    "    return_num = len(pos_list) / len(total) if len(total) != 0 else 0\n",
    "    return return_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in text that has been POS tagged\n",
    "#returns list of word with a particular pos (such as all adjectives)\n",
    "def pos_list(content, pos):\n",
    "    pos_list = []\n",
    "    total = [] #all the words\n",
    "    if isinstance(content, list):\n",
    "        for a,b in content:\n",
    "            if b == pos:\n",
    "                pos_list.append(a)\n",
    "            total.append(a)\n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict classifies news outlets as left or right\n",
    "#left = 1, right = 0\n",
    "my_dict = {'Atlantic': 1,\n",
    "           'Breitbart': 0,\n",
    "           'Business Insider': 1,\n",
    "           'Buzzfeed News': 1,\n",
    "           'CNN': 1,\n",
    "           'Fox News': 0,\n",
    "           'Guardian': 1,\n",
    "           'NPR': 1,\n",
    "           'National Review': 0,\n",
    "           'New York Post': 0,\n",
    "           'New York Times': 1,\n",
    "           'Reuters': 1,\n",
    "           'Talking Points Memo': 1,\n",
    "           'Vox': 1,\n",
    "           'Washington Post': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in publication\n",
    "#retuns L or R classification\n",
    "def mapping(publication):\n",
    "    if publication in my_dict.keys():\n",
    "        return my_dict[publication]\n",
    "    else:\n",
    "        return \"na\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in content\n",
    "#returns boolean denoting whether x from word_in)stuff exists in article?\n",
    "def word_exists(x, list_of_stuff):\n",
    "    return any(word in x for word in list_of_stuff)\n",
    "    \n",
    "    '''\n",
    "    b = false\n",
    "    for a in list_of_stuff:\n",
    "        if (a in x):\n",
    "            b = True\n",
    "    return b'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in list of words (x)\n",
    "#takes in content\n",
    "#returns number of times that the words in list x appear in the string content\n",
    "def word_count(x, content):\n",
    "    count = 0\n",
    "    for a in x:\n",
    "        count += len(re.findall(a, content, flags = re.IGNORECASE))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, Recall, & Overall Accuracy Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints out accuracy\n",
    "def accuracy(confusion_matrix):\n",
    "    TP = confusion_matrix[0,0]\n",
    "    FN = confusion_matrix[0,1]\n",
    "    TN = confusion_matrix[1,1]\n",
    "    FP = confusion_matrix[1,0]\n",
    "    print(\"Overall Accuracy: \" + str((TP + TN) / (TP + FN + TN + FP)))\n",
    "    print('Precision: ' + str(TP / (TP + FP)))\n",
    "    print('Recall: ' + str(TP / (TP + FN)))\n",
    "   #return TP / (TP + FP) if accuracy_type == 'precision' else TP / (TP + FN) if accuracy_type == 'recall' else (TP + TN) / (TP + FN + TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall accuracy\n",
    "def overall_accuracy(confusion_matrix):\n",
    "    TP = confusion_matrix[0,0]\n",
    "    FN = confusion_matrix[0,1]\n",
    "    TN = confusion_matrix[1,1]\n",
    "    FP = confusion_matrix[1,0] \n",
    "    return (TP + TN) / (TP + FN + TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function assumes the dataframe has a column with the same name as the POS\n",
    "# input: dataframe, pos, optional parameter: take out vocab with greater than \"n\" length\n",
    "# create list of words of 'pos' -- iterate through each 'pos' column to take out the vocab with>=5\n",
    "# output -- list of vocab of a certain pos\n",
    "def list_of_vocab(df, pos, word_length = 0):\n",
    "    to_be_put_in_cv = list(itertools.chain.from_iterable(df[pos]))\n",
    "    list_of_vocab = [x for x in to_be_put_in_cv if len(x)>=word_length]\n",
    "    return list_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: list_of_vocab, dataset, n words\n",
    "# create count vectorizer object\n",
    "# list of vocab, \n",
    "# output: array with the top n words\n",
    "def top_n_words_array(list_of_vocab, df, column_name = 'content_clean', n = 50):\n",
    "    vectorizer = CountVectorizer(vocabulary=set(list_of_vocab), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    X = vectorizer.fit_transform(df[column_name].values)\n",
    "    vocab_array = X.toarray()\n",
    "    vocab_array_dataframe = pd.DataFrame(vocab_array, columns=vectorizer.get_feature_names())\n",
    "    word_frequency_agg = pd.DataFrame(vocab_array_dataframe.sum()).reset_index()\n",
    "    word_frequency_agg.columns = ['word','frequency']\n",
    "    word_frequency_agg = word_frequency_agg.sort_values('frequency')\n",
    "    top_n_frequent_words = word_frequency_agg.tail(n)['word']\n",
    "    return top_n_frequent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: array with the top n words, dataset\n",
    "# use ^^ array of top n words as vocab for the next CountVectorizer object\n",
    "# output: CountVectorizer array\n",
    "def final_cv_array(top_n_words_array, df, column_name = 'content_clean'):\n",
    "    vectorizer = CountVectorizer(vocabulary=top_n_words_array, min_df=0, stop_words=frozenset(),token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    X = vectorizer.fit_transform(df[column_name].values)\n",
    "    CV_array = X.toarray()\n",
    "    return CV_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Training, Validation, & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize indexes in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_scrambled = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = df_scrambled.iloc[0:90000, [2,3,4,5,6,7,9]]\n",
    "validation = df_scrambled.iloc[90001:135000, [2,3,4,5,6,7,9]]\n",
    "testing = df_scrambled.iloc[135001:150000, [2,3,4,5,6,7,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79873</th>\n",
       "      <td>Tourists are getting pulled over for staring a...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Lauren Tousignant</td>\n",
       "      <td>2017-02-15</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Theyâre drunk off a natural phenomenon. Icelan...</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14533</th>\n",
       "      <td>Google, Facebook, Twitter Promise to Crack Dow...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Jack Hadfield</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>In a meeting with Amber Rudd, the British Home...</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19786</th>\n",
       "      <td>Judge Jeanine: The Establishment Trying to Tak...</td>\n",
       "      <td>Breitbart</td>\n",
       "      <td>Trent Baker</td>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>On the Saturday broadcast of âJusticeâ on Fox ...</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title    publication  \\\n",
       "79873  Tourists are getting pulled over for staring a...  New York Post   \n",
       "14533  Google, Facebook, Twitter Promise to Crack Dow...      Breitbart   \n",
       "19786  Judge Jeanine: The Establishment Trying to Tak...      Breitbart   \n",
       "\n",
       "                  author        date    year  month  \\\n",
       "79873  Lauren Tousignant  2017-02-15  2017.0    2.0   \n",
       "14533      Jack Hadfield  2017-04-01  2017.0    4.0   \n",
       "19786        Trent Baker  2016-04-09  2016.0    4.0   \n",
       "\n",
       "                                                 content   dataset  \n",
       "79873  Theyâre drunk off a natural phenomenon. Icelan...  training  \n",
       "14533  In a meeting with Amber Rudd, the British Home...  training  \n",
       "19786  On the Saturday broadcast of âJusticeâ on Fox ...  training  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "training['dataset'] = 'training'\n",
    "validation['dataset'] = 'validation'\n",
    "testing['dataset'] = 'testing'\n",
    "training.head(2)\n",
    "\n",
    "df = pd.concat([training, validation, testing])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['leaning'] = df['publication'].apply(lambda x: mapping(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79873</th>\n",
       "      <td>Tourists are getting pulled over for staring a...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Lauren Tousignant</td>\n",
       "      <td>2017-02-15</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Theyâre drunk off a natural phenomenon. Icelan...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title    publication  \\\n",
       "79873  Tourists are getting pulled over for staring a...  New York Post   \n",
       "\n",
       "                  author        date    year  month  \\\n",
       "79873  Lauren Tousignant  2017-02-15  2017.0    2.0   \n",
       "\n",
       "                                                 content   dataset  leaning  \n",
       "79873  Theyâre drunk off a natural phenomenon. Icelan...  training        0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42827</th>\n",
       "      <td>Bryan Cranston said what?! Five Comic-Con OMG ...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Henry Hanks</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>(CNN) Every year, studios, networks and publi...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>âThe Angry Birds Movieâ Nests Atop the Box Off...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Brooks Barnes</td>\n",
       "      <td>2016-05-25</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>An omnipresent,   marketing campaign propelled...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116357</th>\n",
       "      <td>Citigroup fined $28.8 million for harm to home...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Lisa Lambert</td>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Citigroup Inc ( ) mortgage units have been fi...</td>\n",
       "      <td>validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title     publication  \\\n",
       "42827   Bryan Cranston said what?! Five Comic-Con OMG ...             CNN   \n",
       "2798    âThe Angry Birds Movieâ Nests Atop the Box Off...  New York Times   \n",
       "116357  Citigroup fined $28.8 million for harm to home...         Reuters   \n",
       "\n",
       "               author        date    year  month  \\\n",
       "42827     Henry Hanks  2015-07-12  2015.0    7.0   \n",
       "2798    Brooks Barnes  2016-05-25  2016.0    5.0   \n",
       "116357   Lisa Lambert  2017-01-24  2017.0    1.0   \n",
       "\n",
       "                                                  content     dataset  leaning  \n",
       "42827    (CNN) Every year, studios, networks and publi...    training        1  \n",
       "2798    An omnipresent,   marketing campaign propelled...    training        1  \n",
       "116357   Citigroup Inc ( ) mortgage units have been fi...  validation        1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwords = ['trump', 'biden', 'democrat', 'republican', 'left', 'right', 'wing', 'white house', 'election', 'voter',\n",
    "'clinton', 'bush', 'president','democracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_political'] = df['content'].apply(lambda x: word_exists(x, pwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.is_political == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem = df[df.leaning == 0].head(37121)\n",
    "df_rep = df[df.leaning == 1].head(37121)\n",
    "df = pd.concat([df_dem, df_rep]).sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leaning</th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>is_political</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "      <td>34768</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>37120</td>\n",
       "      <td>37121</td>\n",
       "      <td>32613</td>\n",
       "      <td>36266</td>\n",
       "      <td>36266</td>\n",
       "      <td>36266</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "      <td>37121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   leaning  title  publication  author   date   year  month  content  dataset  \\\n",
       "0        0  37121        37121   34768  37121  37121  37121    37121    37121   \n",
       "1        1  37120        37121   32613  36266  36266  36266    37121    37121   \n",
       "\n",
       "   is_political  \n",
       "0         37121  \n",
       "1         37121  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#political articles filtered out with equal number of dem & rep publications\n",
    "df.groupby('leaning').count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['content'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "      <th>is_political</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_lowercase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94841</th>\n",
       "      <td>Milwaukee âin turmoilâ as protesters riot afte...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Sophia Rosenbaum</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Milwaukee was burning Saturday following an   ...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>milwaukee was burning saturday following an   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142565</th>\n",
       "      <td>An eavesdropping Uber driver saved his 16-year...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Avi Selk</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Uber driver Keith Avila picked up a p...</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.074346</td>\n",
       "      <td>uber driver keith avila picked up a p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title      publication  \\\n",
       "94841   Milwaukee âin turmoilâ as protesters riot afte...    New York Post   \n",
       "142565  An eavesdropping Uber driver saved his 16-year...  Washington Post   \n",
       "\n",
       "                  author        date    year  month  \\\n",
       "94841   Sophia Rosenbaum  2016-08-14  2016.0    8.0   \n",
       "142565          Avi Selk  2016-12-30  2016.0   12.0   \n",
       "\n",
       "                                                  content   dataset  leaning  \\\n",
       "94841   Milwaukee was burning Saturday following an   ...  training        0   \n",
       "142565           Uber driver Keith Avila picked up a p...   testing        1   \n",
       "\n",
       "        is_political  sentiment  \\\n",
       "94841           True  -0.154545   \n",
       "142565          True   0.074346   \n",
       "\n",
       "                                        content_lowercase  \n",
       "94841   milwaukee was burning saturday following an   ...  \n",
       "142565           uber driver keith avila picked up a p...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content_lowercase'] = df['content'].str.lower()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "df['lem_content'] = df['content_lowercase'].apply(lambda x:[lem.lemmatize(y, 'n') for y in word_tokenize(x) if y.isalnum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stopwords_removed'] = df['lem_content'].apply(lambda x: stopword_filter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "      <th>is_political</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_lowercase</th>\n",
       "      <th>lem_content</th>\n",
       "      <th>stopwords_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94841</th>\n",
       "      <td>Milwaukee âin turmoilâ as protesters riot afte...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Sophia Rosenbaum</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Milwaukee was burning Saturday following an   ...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>milwaukee was burning saturday following an   ...</td>\n",
       "      <td>[milwaukee, wa, burning, saturday, following, ...</td>\n",
       "      <td>[milwaukee, wa, burning, saturday, following, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title    publication  \\\n",
       "94841  Milwaukee âin turmoilâ as protesters riot afte...  New York Post   \n",
       "\n",
       "                 author        date    year  month  \\\n",
       "94841  Sophia Rosenbaum  2016-08-14  2016.0    8.0   \n",
       "\n",
       "                                                 content   dataset  leaning  \\\n",
       "94841  Milwaukee was burning Saturday following an   ...  training        0   \n",
       "\n",
       "       is_political  sentiment  \\\n",
       "94841          True  -0.154545   \n",
       "\n",
       "                                       content_lowercase  \\\n",
       "94841  milwaukee was burning saturday following an   ...   \n",
       "\n",
       "                                             lem_content  \\\n",
       "94841  [milwaukee, wa, burning, saturday, following, ...   \n",
       "\n",
       "                                       stopwords_removed  \n",
       "94841  [milwaukee, wa, burning, saturday, following, ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(mylist):\n",
    "    new_list = ''\n",
    "    for element in mylist:\n",
    "        new_list += element + ' '\n",
    "    return new_list   \n",
    "\n",
    "df['content_clean'] = df['stopwords_removed'].apply(lambda x: list_to_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "      <th>is_political</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_lowercase</th>\n",
       "      <th>lem_content</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94841</th>\n",
       "      <td>Milwaukee âin turmoilâ as protesters riot afte...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Sophia Rosenbaum</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Milwaukee was burning Saturday following an   ...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>milwaukee was burning saturday following an   ...</td>\n",
       "      <td>[milwaukee, wa, burning, saturday, following, ...</td>\n",
       "      <td>[milwaukee, wa, burning, saturday, following, ...</td>\n",
       "      <td>milwaukee wa burning saturday following shooti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142565</th>\n",
       "      <td>An eavesdropping Uber driver saved his 16-year...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Avi Selk</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Uber driver Keith Avila picked up a p...</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.074346</td>\n",
       "      <td>uber driver keith avila picked up a p...</td>\n",
       "      <td>[uber, driver, keith, avila, picked, up, a, pa...</td>\n",
       "      <td>[uber, driver, keith, avila, picked, passenger...</td>\n",
       "      <td>uber driver keith avila picked passenger looke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136099</th>\n",
       "      <td>ACT essay scores are inexplicably low, causing...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Nick Anderson</td>\n",
       "      <td>2016-02-12</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Many students are in an uproar over a cha...</td>\n",
       "      <td>training</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.157549</td>\n",
       "      <td>many students are in an uproar over a cha...</td>\n",
       "      <td>[many, student, are, in, an, uproar, over, a, ...</td>\n",
       "      <td>[many, student, uproar, change, act, ha, yield...</td>\n",
       "      <td>many student uproar change act ha yielded call...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title      publication  \\\n",
       "94841   Milwaukee âin turmoilâ as protesters riot afte...    New York Post   \n",
       "142565  An eavesdropping Uber driver saved his 16-year...  Washington Post   \n",
       "136099  ACT essay scores are inexplicably low, causing...  Washington Post   \n",
       "\n",
       "                  author        date    year  month  \\\n",
       "94841   Sophia Rosenbaum  2016-08-14  2016.0    8.0   \n",
       "142565          Avi Selk  2016-12-30  2016.0   12.0   \n",
       "136099     Nick Anderson  2016-02-12  2016.0    2.0   \n",
       "\n",
       "                                                  content   dataset  leaning  \\\n",
       "94841   Milwaukee was burning Saturday following an   ...  training        0   \n",
       "142565           Uber driver Keith Avila picked up a p...   testing        1   \n",
       "136099       Many students are in an uproar over a cha...  training        1   \n",
       "\n",
       "        is_political  sentiment  \\\n",
       "94841           True  -0.154545   \n",
       "142565          True   0.074346   \n",
       "136099          True   0.157549   \n",
       "\n",
       "                                        content_lowercase  \\\n",
       "94841   milwaukee was burning saturday following an   ...   \n",
       "142565           uber driver keith avila picked up a p...   \n",
       "136099       many students are in an uproar over a cha...   \n",
       "\n",
       "                                              lem_content  \\\n",
       "94841   [milwaukee, wa, burning, saturday, following, ...   \n",
       "142565  [uber, driver, keith, avila, picked, up, a, pa...   \n",
       "136099  [many, student, are, in, an, uproar, over, a, ...   \n",
       "\n",
       "                                        stopwords_removed  \\\n",
       "94841   [milwaukee, wa, burning, saturday, following, ...   \n",
       "142565  [uber, driver, keith, avila, picked, passenger...   \n",
       "136099  [many, student, uproar, change, act, ha, yield...   \n",
       "\n",
       "                                            content_clean  \n",
       "94841   milwaukee wa burning saturday following shooti...  \n",
       "142565  uber driver keith avila picked passenger looke...  \n",
       "136099  many student uproar change act ha yielded call...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "df['POS_tagging'] = df['stopwords_removed'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "      <th>is_political</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_lowercase</th>\n",
       "      <th>lem_content</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>POS_tagging</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94841</th>\n",
       "      <td>Milwaukee âin turmoilâ as protesters riot afte...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Sophia Rosenbaum</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Milwaukee was burning Saturday following an   ...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.154545</td>\n",
       "      <td>milwaukee was burning saturday following an   ...</td>\n",
       "      <td>[milwaukee, wa, burning, saturday, following, ...</td>\n",
       "      <td>[milwaukee, wa, burning, saturday, following, ...</td>\n",
       "      <td>milwaukee wa burning saturday following shooti...</td>\n",
       "      <td>[(milwaukee, NN), (wa, NN), (burning, VBG), (s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142565</th>\n",
       "      <td>An eavesdropping Uber driver saved his 16-year...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>Avi Selk</td>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Uber driver Keith Avila picked up a p...</td>\n",
       "      <td>testing</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.074346</td>\n",
       "      <td>uber driver keith avila picked up a p...</td>\n",
       "      <td>[uber, driver, keith, avila, picked, up, a, pa...</td>\n",
       "      <td>[uber, driver, keith, avila, picked, passenger...</td>\n",
       "      <td>uber driver keith avila picked passenger looke...</td>\n",
       "      <td>[(uber, JJ), (driver, NN), (keith, NN), (avila...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title      publication  \\\n",
       "94841   Milwaukee âin turmoilâ as protesters riot afte...    New York Post   \n",
       "142565  An eavesdropping Uber driver saved his 16-year...  Washington Post   \n",
       "\n",
       "                  author        date    year  month  \\\n",
       "94841   Sophia Rosenbaum  2016-08-14  2016.0    8.0   \n",
       "142565          Avi Selk  2016-12-30  2016.0   12.0   \n",
       "\n",
       "                                                  content   dataset  leaning  \\\n",
       "94841   Milwaukee was burning Saturday following an   ...  training        0   \n",
       "142565           Uber driver Keith Avila picked up a p...   testing        1   \n",
       "\n",
       "        is_political  sentiment  \\\n",
       "94841           True  -0.154545   \n",
       "142565          True   0.074346   \n",
       "\n",
       "                                        content_lowercase  \\\n",
       "94841   milwaukee was burning saturday following an   ...   \n",
       "142565           uber driver keith avila picked up a p...   \n",
       "\n",
       "                                              lem_content  \\\n",
       "94841   [milwaukee, wa, burning, saturday, following, ...   \n",
       "142565  [uber, driver, keith, avila, picked, up, a, pa...   \n",
       "\n",
       "                                        stopwords_removed  \\\n",
       "94841   [milwaukee, wa, burning, saturday, following, ...   \n",
       "142565  [uber, driver, keith, avila, picked, passenger...   \n",
       "\n",
       "                                            content_clean  \\\n",
       "94841   milwaukee wa burning saturday following shooti...   \n",
       "142565  uber driver keith avila picked passenger looke...   \n",
       "\n",
       "                                              POS_tagging  \n",
       "94841   [(milwaukee, NN), (wa, NN), (burning, VBG), (s...  \n",
       "142565  [(uber, JJ), (driver, NN), (keith, NN), (avila...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certain Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjective, i.e. \"small\"\n",
    "df['JJ'] = df['POS_tagging'].apply(lambda x: pos_list(x, 'JJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#singular noun, i.e. \"cheese\"\n",
    "df['NN'] = df['POS_tagging'].apply(lambda x: pos_list(x, 'NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base form verb, i.e. \"run\"\n",
    "df['VB'] = df['POS_tagging'].apply(lambda x: pos_list(x, 'VB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cardinal digit (numbers used to count \"how many\")\n",
    "df['CD'] = df['POS_tagging'].apply(lambda x: pos_list(x, 'CD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj and noun\n",
    "df['JJ, NN'] = df['POS_tagging'].apply(lambda x: pos_list(x, 'JJ' or 'NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj and noun\n",
    "df['NN, VB'] = df['POS_tagging'].apply(lambda x: pos_list(x, 'NN' or 'VB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of adjs, i.e. \"small\"\n",
    "df['adj_proportion'] = df['POS_tagging'].apply(lambda x: pos_proportion(x, 'JJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of singular nouns, i.e. \"cheese\"\n",
    "df['noun_proportion'] = df['POS_tagging'].apply(lambda x: pos_proportion(x, 'NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of base form verbs, i.e. \"run\"\n",
    "df['verb_proportion'] = df['POS_tagging'].apply(lambda x: pos_proportion(x, 'VB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion of cardinal digits (numbers used to count \"how many\")\n",
    "df['c_digit_proportion'] = df['POS_tagging'].apply(lambda x: pos_proportion(x, 'CD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "      <th>is_political</th>\n",
       "      <th>...</th>\n",
       "      <th>JJ</th>\n",
       "      <th>NN</th>\n",
       "      <th>VB</th>\n",
       "      <th>CD</th>\n",
       "      <th>JJ, NN</th>\n",
       "      <th>NN, VB</th>\n",
       "      <th>adj_proportion</th>\n",
       "      <th>noun_proportion</th>\n",
       "      <th>verb_proportion</th>\n",
       "      <th>c_digit_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94841</th>\n",
       "      <td>Milwaukee âin turmoilâ as protesters riot afte...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Sophia Rosenbaum</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Milwaukee was burning Saturday following an   ...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>[ignited, violent, numerous, brick, deadly, su...</td>\n",
       "      <td>[milwaukee, wa, saturday, protest, fire, build...</td>\n",
       "      <td>[get]</td>\n",
       "      <td>[100, one, two, one, five, 24]</td>\n",
       "      <td>[ignited, violent, numerous, brick, deadly, su...</td>\n",
       "      <td>[milwaukee, wa, saturday, protest, fire, build...</td>\n",
       "      <td>0.132597</td>\n",
       "      <td>0.552486</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.033149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title    publication  \\\n",
       "94841  Milwaukee âin turmoilâ as protesters riot afte...  New York Post   \n",
       "\n",
       "                 author        date    year  month  \\\n",
       "94841  Sophia Rosenbaum  2016-08-14  2016.0    8.0   \n",
       "\n",
       "                                                 content   dataset  leaning  \\\n",
       "94841  Milwaukee was burning Saturday following an   ...  training        0   \n",
       "\n",
       "       is_political  ...                                                 JJ  \\\n",
       "94841          True  ...  [ignited, violent, numerous, brick, deadly, su...   \n",
       "\n",
       "                                                      NN     VB  \\\n",
       "94841  [milwaukee, wa, saturday, protest, fire, build...  [get]   \n",
       "\n",
       "                                   CD  \\\n",
       "94841  [100, one, two, one, five, 24]   \n",
       "\n",
       "                                                  JJ, NN  \\\n",
       "94841  [ignited, violent, numerous, brick, deadly, su...   \n",
       "\n",
       "                                                  NN, VB adj_proportion  \\\n",
       "94841  [milwaukee, wa, saturday, protest, fire, build...       0.132597   \n",
       "\n",
       "      noun_proportion verb_proportion c_digit_proportion  \n",
       "94841        0.552486        0.005525           0.033149  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Political Organizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left political organizations\n",
    "dem_organization_words = [\n",
    "    'AAPI Democrats', 'AfricanAmerican Dems','College Democrats','High School Democrats of America',\n",
    "    'LGBT Democrats','Los DemÃ³cratas', 'National Federation of Democratic Women',\n",
    "    'National Jewish Democratic Council','Stonewall Democrats','Young Democrats of America','DNC Women', \n",
    "    'Center for American Progress', 'Blue Dog Coalition', 'Democracy for America','ActBlue',\n",
    "    'America Votes','Democrats for Life of America','New Democrat Coalition','New Democrat Network',\n",
    "    'Progressive Caucus', 'Progressive Change Campaign Committee','Progressive Democrats of America',\n",
    "    'Progressive Policy Institute','Moveon.org','America Coming Together','Democratic Leadership Council','Democrats for Life of America',\n",
    "    'Democratic Congressional Campaign Committee','Democratic Governors Association',\n",
    "    'Democratic National Committee','Democratic Senatorial Campaign Committee','Democrats Abroad',\n",
    "    'National Conference of Democratic Mayors','Democratic Legislative Campaign Committee',\n",
    "    'Democratic Attorneys General Association','Democratic Association of Secretaries of State',\n",
    "    'National Democratic County Officials','Democratic Municipal Officials', \n",
    "]\n",
    "df['dem_organization_count'] = df['content'].apply(lambda x: word_count(dem_organization_words, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#right political organizations\n",
    "rep_organization_words = [\n",
    "    'Americans for a Republican Majority','California Congress of Republicans','California Republican Assembly',\n",
    "    'Capitol Hill Club','Republican Majority for Choice','Republicans for Choice','College Republicans',\n",
    "    'Republican Conference of the United States House of Representatives','Republican Conference of the United States Senate',\n",
    "    'National Republican Congressional Committee','Congressional Hispanic Conference','Congressional Institute',\n",
    "    'ConservAmerica','Courageous Conservatives PAC','Delegates Unbound','Freedom Caucus','Georgia Teen Republicans',\n",
    "    'GOPAC','Republican Governors Association','Hollywood Congress of Republicans','Hoover League','Huck PAC',\n",
    "    'Idaho Federation of Reagan Republicans','International Republican Institute','Republican Jewish Coalition',\n",
    "    'Kansas Traditional Republican Majority','Republican Leadership Council','Liberty Caucus','Republican National Coalition for Life',\n",
    "    'LincolnâRoosevelt League','Log Cabin Republicans','Republican Main Street Partnership','Mainstream Republicans of Washington','National Black Republican Association',\n",
    "    'Republican National Committee','National Council for a New America','National Federation of Republican Assemblies',\n",
    "    'National Federation of Republican Women','Republican Liberty Caucus','Republican National Hispanic Assembly',\n",
    "    'Republican National Lawyers Association','Republican State Leadership Committee','Republicans Abroad',\n",
    "    'Republicans Abroad Norway','Republicans for Immigration Reform','Republicans Overseas','RightChange.com',\n",
    "    'RightNOW Women','Ripon Society','SarahPAC','National Republican Senatorial Committee','Republican Study Committee',\n",
    "    'Tea Party Caucus','Teen Age Republicans','Texans for a Republican Majority','The Tuesday Group','Republican Unity Coalition',\n",
    "    'The Wish List','Young Republicans',\n",
    "]\n",
    "df['rep_organization_count'] = df['content'].apply(lambda x: word_count(rep_organization_words, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>content</th>\n",
       "      <th>dataset</th>\n",
       "      <th>leaning</th>\n",
       "      <th>is_political</th>\n",
       "      <th>...</th>\n",
       "      <th>VB</th>\n",
       "      <th>CD</th>\n",
       "      <th>JJ, NN</th>\n",
       "      <th>NN, VB</th>\n",
       "      <th>adj_proportion</th>\n",
       "      <th>noun_proportion</th>\n",
       "      <th>verb_proportion</th>\n",
       "      <th>c_digit_proportion</th>\n",
       "      <th>dem_organization_count</th>\n",
       "      <th>rep_organization_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94841</th>\n",
       "      <td>Milwaukee âin turmoilâ as protesters riot afte...</td>\n",
       "      <td>New York Post</td>\n",
       "      <td>Sophia Rosenbaum</td>\n",
       "      <td>2016-08-14</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Milwaukee was burning Saturday following an   ...</td>\n",
       "      <td>training</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>[get]</td>\n",
       "      <td>[100, one, two, one, five, 24]</td>\n",
       "      <td>[ignited, violent, numerous, brick, deadly, su...</td>\n",
       "      <td>[milwaukee, wa, saturday, protest, fire, build...</td>\n",
       "      <td>0.132597</td>\n",
       "      <td>0.552486</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.033149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title    publication  \\\n",
       "94841  Milwaukee âin turmoilâ as protesters riot afte...  New York Post   \n",
       "\n",
       "                 author        date    year  month  \\\n",
       "94841  Sophia Rosenbaum  2016-08-14  2016.0    8.0   \n",
       "\n",
       "                                                 content   dataset  leaning  \\\n",
       "94841  Milwaukee was burning Saturday following an   ...  training        0   \n",
       "\n",
       "       is_political  ...     VB                              CD  \\\n",
       "94841          True  ...  [get]  [100, one, two, one, five, 24]   \n",
       "\n",
       "                                                  JJ, NN  \\\n",
       "94841  [ignited, violent, numerous, brick, deadly, su...   \n",
       "\n",
       "                                                  NN, VB adj_proportion  \\\n",
       "94841  [milwaukee, wa, saturday, protest, fire, build...       0.132597   \n",
       "\n",
       "      noun_proportion verb_proportion c_digit_proportion  \\\n",
       "94841        0.552486        0.005525           0.033149   \n",
       "\n",
       "      dem_organization_count rep_organization_count  \n",
       "94841                      0                      0  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>leaning</th>\n",
       "      <th>is_political</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>adj_proportion</th>\n",
       "      <th>noun_proportion</th>\n",
       "      <th>verb_proportion</th>\n",
       "      <th>c_digit_proportion</th>\n",
       "      <th>dem_organization_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rep_organization_count</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.328496</td>\n",
       "      <td>5.542604</td>\n",
       "      <td>0.499048</td>\n",
       "      <td>True</td>\n",
       "      <td>0.077854</td>\n",
       "      <td>0.191267</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.020540</td>\n",
       "      <td>0.033415</td>\n",
       "      <td>0.023181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.250774</td>\n",
       "      <td>6.055728</td>\n",
       "      <td>0.525076</td>\n",
       "      <td>True</td>\n",
       "      <td>0.085585</td>\n",
       "      <td>0.200045</td>\n",
       "      <td>0.472505</td>\n",
       "      <td>0.022180</td>\n",
       "      <td>0.028512</td>\n",
       "      <td>0.102584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.430622</td>\n",
       "      <td>5.473684</td>\n",
       "      <td>0.614679</td>\n",
       "      <td>True</td>\n",
       "      <td>0.095734</td>\n",
       "      <td>0.199892</td>\n",
       "      <td>0.473245</td>\n",
       "      <td>0.025996</td>\n",
       "      <td>0.024250</td>\n",
       "      <td>0.082569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.550562</td>\n",
       "      <td>5.078652</td>\n",
       "      <td>0.445652</td>\n",
       "      <td>True</td>\n",
       "      <td>0.084243</td>\n",
       "      <td>0.190999</td>\n",
       "      <td>0.486952</td>\n",
       "      <td>0.026056</td>\n",
       "      <td>0.024515</td>\n",
       "      <td>0.141304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.622222</td>\n",
       "      <td>5.155556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>True</td>\n",
       "      <td>0.089851</td>\n",
       "      <td>0.187613</td>\n",
       "      <td>0.495333</td>\n",
       "      <td>0.026813</td>\n",
       "      <td>0.026284</td>\n",
       "      <td>0.022222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016.476190</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>True</td>\n",
       "      <td>0.098957</td>\n",
       "      <td>0.194482</td>\n",
       "      <td>0.467591</td>\n",
       "      <td>0.025469</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016.809524</td>\n",
       "      <td>3.904762</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.081694</td>\n",
       "      <td>0.186973</td>\n",
       "      <td>0.479589</td>\n",
       "      <td>0.031378</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016.700000</td>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>True</td>\n",
       "      <td>0.085841</td>\n",
       "      <td>0.179717</td>\n",
       "      <td>0.499744</td>\n",
       "      <td>0.033647</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016.916667</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.078634</td>\n",
       "      <td>0.191342</td>\n",
       "      <td>0.488665</td>\n",
       "      <td>0.027975</td>\n",
       "      <td>0.018397</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016.833333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>True</td>\n",
       "      <td>0.053449</td>\n",
       "      <td>0.188148</td>\n",
       "      <td>0.493743</td>\n",
       "      <td>0.027643</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016.800000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>0.065654</td>\n",
       "      <td>0.187019</td>\n",
       "      <td>0.484914</td>\n",
       "      <td>0.027391</td>\n",
       "      <td>0.022202</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016.333333</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>0.079962</td>\n",
       "      <td>0.212340</td>\n",
       "      <td>0.459867</td>\n",
       "      <td>0.019038</td>\n",
       "      <td>0.023815</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.195609</td>\n",
       "      <td>0.453666</td>\n",
       "      <td>0.031754</td>\n",
       "      <td>0.029217</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100822</td>\n",
       "      <td>0.180001</td>\n",
       "      <td>0.490695</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.111548</td>\n",
       "      <td>0.184530</td>\n",
       "      <td>0.506077</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.073023</td>\n",
       "      <td>0.182521</td>\n",
       "      <td>0.480052</td>\n",
       "      <td>0.027923</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.105580</td>\n",
       "      <td>0.196110</td>\n",
       "      <td>0.445705</td>\n",
       "      <td>0.029173</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.049226</td>\n",
       "      <td>0.191275</td>\n",
       "      <td>0.558725</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.011745</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.097947</td>\n",
       "      <td>0.200651</td>\n",
       "      <td>0.475054</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.018438</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.114048</td>\n",
       "      <td>0.206178</td>\n",
       "      <td>0.469297</td>\n",
       "      <td>0.026424</td>\n",
       "      <td>0.018236</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.115797</td>\n",
       "      <td>0.198889</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.113951</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.476618</td>\n",
       "      <td>0.017957</td>\n",
       "      <td>0.029929</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               year      month   leaning  is_political  \\\n",
       "rep_organization_count                                                   \n",
       "0                       2016.328496   5.542604  0.499048          True   \n",
       "1                       2016.250774   6.055728  0.525076          True   \n",
       "2                       2016.430622   5.473684  0.614679          True   \n",
       "3                       2016.550562   5.078652  0.445652          True   \n",
       "4                       2016.622222   5.155556  0.555556          True   \n",
       "5                       2016.476190   5.714286  0.571429          True   \n",
       "6                       2016.809524   3.904762  0.583333          True   \n",
       "7                       2016.700000   3.900000  0.636364          True   \n",
       "8                       2016.916667   3.750000  0.750000          True   \n",
       "9                       2016.833333   3.666667  0.833333          True   \n",
       "10                      2016.800000   4.000000  0.666667          True   \n",
       "11                      2016.333333   8.000000  0.666667          True   \n",
       "12                      2017.000000   3.500000  0.500000          True   \n",
       "13                      2016.666667   3.000000  0.666667          True   \n",
       "14                      2017.000000   3.000000  1.000000          True   \n",
       "15                      2016.500000   7.000000  1.000000          True   \n",
       "16                      2017.000000   4.000000  1.000000          True   \n",
       "19                      2016.000000   5.000000  0.000000          True   \n",
       "20                      2017.000000   3.000000  1.000000          True   \n",
       "26                      2016.000000  12.000000  0.000000          True   \n",
       "32                      2017.000000   3.000000  1.000000          True   \n",
       "44                      2016.000000   8.000000  0.000000          True   \n",
       "\n",
       "                        sentiment  adj_proportion  noun_proportion  \\\n",
       "rep_organization_count                                               \n",
       "0                        0.077854        0.191267         0.464741   \n",
       "1                        0.085585        0.200045         0.472505   \n",
       "2                        0.095734        0.199892         0.473245   \n",
       "3                        0.084243        0.190999         0.486952   \n",
       "4                        0.089851        0.187613         0.495333   \n",
       "5                        0.098957        0.194482         0.467591   \n",
       "6                        0.081694        0.186973         0.479589   \n",
       "7                        0.085841        0.179717         0.499744   \n",
       "8                        0.078634        0.191342         0.488665   \n",
       "9                        0.053449        0.188148         0.493743   \n",
       "10                       0.065654        0.187019         0.484914   \n",
       "11                       0.079962        0.212340         0.459867   \n",
       "12                       0.088358        0.195609         0.453666   \n",
       "13                       0.100822        0.180001         0.490695   \n",
       "14                       0.111548        0.184530         0.506077   \n",
       "15                       0.073023        0.182521         0.480052   \n",
       "16                       0.105580        0.196110         0.445705   \n",
       "19                       0.049226        0.191275         0.558725   \n",
       "20                       0.097947        0.200651         0.475054   \n",
       "26                       0.114048        0.206178         0.469297   \n",
       "32                       0.115797        0.198889         0.493333   \n",
       "44                       0.113951        0.181818         0.476618   \n",
       "\n",
       "                        verb_proportion  c_digit_proportion  \\\n",
       "rep_organization_count                                        \n",
       "0                              0.020540            0.033415   \n",
       "1                              0.022180            0.028512   \n",
       "2                              0.025996            0.024250   \n",
       "3                              0.026056            0.024515   \n",
       "4                              0.026813            0.026284   \n",
       "5                              0.025469            0.024348   \n",
       "6                              0.031378            0.020686   \n",
       "7                              0.033647            0.024735   \n",
       "8                              0.027975            0.018397   \n",
       "9                              0.027643            0.020542   \n",
       "10                             0.027391            0.022202   \n",
       "11                             0.019038            0.023815   \n",
       "12                             0.031754            0.029217   \n",
       "13                             0.024176            0.012484   \n",
       "14                             0.016575            0.014365   \n",
       "15                             0.027923            0.009967   \n",
       "16                             0.029173            0.030794   \n",
       "19                             0.003356            0.011745   \n",
       "20                             0.024946            0.018438   \n",
       "26                             0.026424            0.018236   \n",
       "32                             0.027778            0.016667   \n",
       "44                             0.017957            0.029929   \n",
       "\n",
       "                        dem_organization_count  \n",
       "rep_organization_count                          \n",
       "0                                     0.023181  \n",
       "1                                     0.102584  \n",
       "2                                     0.082569  \n",
       "3                                     0.141304  \n",
       "4                                     0.022222  \n",
       "5                                     0.000000  \n",
       "6                                     0.000000  \n",
       "7                                     0.090909  \n",
       "8                                     0.000000  \n",
       "9                                     0.000000  \n",
       "10                                    0.000000  \n",
       "11                                    0.000000  \n",
       "12                                    0.000000  \n",
       "13                                    0.000000  \n",
       "14                                    0.000000  \n",
       "15                                    0.000000  \n",
       "16                                    0.000000  \n",
       "19                                    0.000000  \n",
       "20                                    0.000000  \n",
       "26                                    0.000000  \n",
       "32                                    0.000000  \n",
       "44                                    0.000000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['dem_organization_count']).mean()\n",
    "df.groupby(['rep_organization_count']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "testing        3962\n",
       "training      46752\n",
       "validation    23528\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['dataset']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df[df.dataset == 'validation']\n",
    "df_training = df[df.dataset == 'training']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#done in function, bottom are notes\n",
    "to_be_put_in_cv_JJ = list(itertools.chain.from_iterable(df['JJ']))\n",
    "long_word_JJ = [x for x in to_be_put_in_cv_JJ if len(x)>=5]\n",
    "a = '''\n",
    "to_be_put_in_cv_NN = list(itertools.chain.from_iterable(df['NN']))\n",
    "long_word_NN = [x for x in to_be_put_in_cv_NN if len(x)>=5]\n",
    "to_be_put_in_cv_VB = list(itertools.chain.from_iterable(df['VB']))\n",
    "long_word_VB = [x for x in to_be_put_in_cv_VB if len(x)>=5]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JJ Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JJ Words is for reference\n",
    "#Rest will be completed using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=set(long_word_JJ), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = vectorizer.fit_transform(df_training['content_clean'].values)\n",
    "JJ_array_training = X.toarray()\n",
    "JJ_array_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "JJ_array_dataframe_training = pd.DataFrame(JJ_array_training, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000th</th>\n",
       "      <th>040th</th>\n",
       "      <th>080th</th>\n",
       "      <th>085total</th>\n",
       "      <th>100th</th>\n",
       "      <th>100yearsstrong</th>\n",
       "      <th>105th</th>\n",
       "      <th>110th</th>\n",
       "      <th>114th</th>\n",
       "      <th>115th</th>\n",
       "      <th>...</th>\n",
       "      <th>Ø§ÙØ¬ÙØ§Ø±Ù</th>\n",
       "      <th>Ø§ÙÙÙÙØºØ±Ø³</th>\n",
       "      <th>Ø§ÙÙÙÙØ¯</th>\n",
       "      <th>Ø±ÙØ¶Ø§Ù</th>\n",
       "      <th>Ø¹Ø¨Ø¯Ø§ÙØ¹Ø²ÙØ²</th>\n",
       "      <th>ÙØ§ØµØ§Ø¨</th>\n",
       "      <th>ÙÙØ§Ø¬Ø²Ø§Ø¡Ø§ÙØ¥Ø­Ø³Ø§Ù</th>\n",
       "      <th>á´á´sá´á´Êsá´É´á´á´Ê</th>\n",
       "      <th>ã¾ã£ããå°çããp</th>\n",
       "      <th>ï½ï½ï½ï½ï½ï½</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46747</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46748</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46749</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46751</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46752 rows Ã 78117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       000th  040th  080th  085total  100th  100yearsstrong  105th  110th  \\\n",
       "0          0      0      0         0      0               0      0      0   \n",
       "1          0      0      0         0      0               0      0      0   \n",
       "2          0      0      0         0      0               0      0      0   \n",
       "3          0      0      0         0      0               0      0      0   \n",
       "4          0      0      0         0      0               0      0      0   \n",
       "...      ...    ...    ...       ...    ...             ...    ...    ...   \n",
       "46747      0      0      0         0      0               0      0      0   \n",
       "46748      0      0      0         0      0               0      0      0   \n",
       "46749      0      0      0         0      0               0      0      0   \n",
       "46750      0      0      0         0      0               0      0      0   \n",
       "46751      0      0      0         0      0               0      0      0   \n",
       "\n",
       "       114th  115th  ...  Ø§ÙØ¬ÙØ§Ø±Ù  Ø§ÙÙÙÙØºØ±Ø³  Ø§ÙÙÙÙØ¯  Ø±ÙØ¶Ø§Ù  Ø¹Ø¨Ø¯Ø§ÙØ¹Ø²ÙØ²  ÙØ§ØµØ§Ø¨  \\\n",
       "0          0      0  ...        0         0       0      0          0      0   \n",
       "1          0      0  ...        0         0       0      0          0      0   \n",
       "2          0      0  ...        0         0       0      0          0      0   \n",
       "3          0      0  ...        0         0       0      0          0      0   \n",
       "4          0      0  ...        0         0       0      0          0      0   \n",
       "...      ...    ...  ...      ...       ...     ...    ...        ...    ...   \n",
       "46747      0      0  ...        0         0       0      0          0      0   \n",
       "46748      0      0  ...        0         0       0      0          0      0   \n",
       "46749      0      0  ...        0         0       0      0          0      0   \n",
       "46750      0      0  ...        0         0       0      0          0      0   \n",
       "46751      0      0  ...        0         0       0      0          0      0   \n",
       "\n",
       "       ÙÙØ§Ø¬Ø²Ø§Ø¡Ø§ÙØ¥Ø­Ø³Ø§Ù  á´á´sá´á´Êsá´É´á´á´Ê  ã¾ã£ããå°çããp  ï½ï½ï½ï½ï½ï½  \n",
       "0                   0             0          0       0  \n",
       "1                   0             0          0       0  \n",
       "2                   0             0          0       0  \n",
       "3                   0             0          0       0  \n",
       "4                   0             0          0       0  \n",
       "...               ...           ...        ...     ...  \n",
       "46747               0             0          0       0  \n",
       "46748               0             0          0       0  \n",
       "46749               0             0          0       0  \n",
       "46750               0             0          0       0  \n",
       "46751               0             0          0       0  \n",
       "\n",
       "[46752 rows x 78117 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JJ_array_dataframe_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000th</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>040th</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>080th</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>085total</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100th</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78112</th>\n",
       "      <td>ÙØ§ØµØ§Ø¨</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78113</th>\n",
       "      <td>ÙÙØ§Ø¬Ø²Ø§Ø¡Ø§ÙØ¥Ø­Ø³Ø§Ù</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78114</th>\n",
       "      <td>á´á´sá´á´Êsá´É´á´á´Ê</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78115</th>\n",
       "      <td>ã¾ã£ããå°çããp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78116</th>\n",
       "      <td>ï½ï½ï½ï½ï½ï½</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78117 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                index    0\n",
       "0               000th   12\n",
       "1               040th    0\n",
       "2               080th    2\n",
       "3            085total    0\n",
       "4               100th  115\n",
       "...               ...  ...\n",
       "78112           ÙØ§ØµØ§Ø¨    0\n",
       "78113  ÙÙØ§Ø¬Ø²Ø§Ø¡Ø§ÙØ¥Ø­Ø³Ø§Ù    1\n",
       "78114    á´á´sá´á´Êsá´É´á´á´Ê    1\n",
       "78115       ã¾ã£ããå°çããp    1\n",
       "78116          ï½ï½ï½ï½ï½ï½    1\n",
       "\n",
       "[78117 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_JJ_agg_training = pd.DataFrame(JJ_array_dataframe_training.sum()).reset_index()\n",
    "word_frequency_JJ_agg_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7043           better\n",
       "65874         student\n",
       "37097           later\n",
       "38580          little\n",
       "24495           found\n",
       "27487           great\n",
       "68884          though\n",
       "13563    conservative\n",
       "4381            asked\n",
       "7480            black\n",
       "51977           place\n",
       "16512      democratic\n",
       "22841         federal\n",
       "70963         twitter\n",
       "11197          change\n",
       "9892           called\n",
       "16511        democrat\n",
       "11648           child\n",
       "54639        question\n",
       "10064       candidate\n",
       "53305    presidential\n",
       "33692           issue\n",
       "4727           attack\n",
       "75772      washington\n",
       "4089           around\n",
       "75326           voter\n",
       "66631         support\n",
       "41795          medium\n",
       "30004         hillary\n",
       "2524          america\n",
       "52308           point\n",
       "24375          former\n",
       "56566          report\n",
       "54091          public\n",
       "52381          police\n",
       "48219        official\n",
       "72729          united\n",
       "45136        national\n",
       "52417       political\n",
       "76408           white\n",
       "18339          donald\n",
       "50990         percent\n",
       "47886           obama\n",
       "68790           think\n",
       "57290           right\n",
       "56630      republican\n",
       "23541           first\n",
       "2530         american\n",
       "12451         clinton\n",
       "70503           trump\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency_JJ_agg_training.columns = ['word', 'frequency']\n",
    "word_frequency_JJ_agg_training = word_frequency_JJ_agg_training.sort_values('frequency')\n",
    "top_50_frequent_JJ_words_training = word_frequency_JJ_agg_training[word_frequency_JJ_agg_training['word'].apply(lambda x: len(x) >= 5)].tail(50)['word']\n",
    "top_50_frequent_JJ_words_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 1, 18,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  5,  6,  7],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0,  1, ...,  0,  0, 30],\n",
       "       [ 0,  0,  0, ...,  1,  0,  2]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=top_50_frequent_JJ_words_training, min_df=0, stop_words=frozenset(), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = vectorizer.fit_transform(df_training['content_clean'].values)\n",
    "JJ_array_training = X.toarray()\n",
    "JJ_array_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46752, 50)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JJ_array_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for reference!\n",
    "#def list_of_vocab(df, pos, word_length = 0):\n",
    "#def top_n_words_array(list_of_vocab, df, column_name = 'content_clean', n = 50):\n",
    "#def final_cv_array(top_n_words_array, df, column_name = 'content_clean'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'top_n_words_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-05b3f8845cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#building vocab, only works on df_training, (entire df has nan values, so this function doesn't work there)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtop_50_frequent_NN_words_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_n_words_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content_clean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'top_n_words_array' is not defined"
     ]
    }
   ],
   "source": [
    "#building vocab, only works on df_training, (entire df has nan values, so this function doesn't work there)\n",
    "top_50_frequent_NN_words_training = top_n_words_array(list_of_vocab(df, 'NN', 5), df_training, 'content_clean', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VB Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building vocab, only works on df_training\n",
    "top_50_frequent_VB_words_training = top_n_words_array(list_of_vocab(df, 'VB', 5), df_training, 'content_clean', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CD Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building vocab, only works on df_training\n",
    "top_50_frequent_CD_words_training = top_n_words_array(list_of_vocab(df, 'CD'), df_training, 'content_clean', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN and VB Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building vocab, only works on df_training\n",
    "top_50_frequent_NN_VB_words_training = top_n_words_array(list_of_vocab(df, 'NN, VB'), df_training, 'content_clean', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JJ and NN Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building vocab, only works on df_training\n",
    "top_50_frequent_JJ_NN_words_training = top_n_words_array(list_of_vocab(df, 'JJ, NN'), df_training, 'content_clean', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['dataset']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_training, JJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = df_training[['sentiment', 'adj_proportion', 'noun_proportion', 'verb_proportion', 'c_digit_proportion']].to_numpy()\n",
    "y_train = df_training['leaning'].apply(lambda x: int(x))\n",
    "X_vectorizer_training = JJ_array_training\n",
    "X_train = np.concatenate((X_train_features, X_vectorizer_training), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = logreg.predict(X_train)\n",
    "confusion_matrix_training_JJ = confusion_matrix(y_train, y_pred)\n",
    "confusion_matrix_training_JJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_training_JJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_validation, JJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for reference only, but can be done using function\n",
    "vectorizer = CountVectorizer(vocabulary=top_50_frequent_JJ_words_training, min_df=0, stop_words=frozenset(), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = vectorizer.fit_transform(df_validation['content_clean'].values)\n",
    "JJ_array_validation = X.toarray()\n",
    "JJ_array_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_features = df_validation[['sentiment', 'adj_proportion', 'noun_proportion', 'verb_proportion', 'c_digit_proportion']].to_numpy()\n",
    "y_val = df_validation['leaning'].apply(lambda x: int(x))\n",
    "X_vectorizer_validation = JJ_array_validation\n",
    "X_val = np.concatenate((X_val_features, X_vectorizer_validation), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = logreg.predict(X_val)\n",
    "confusion_matrix_val_JJ = confusion_matrix(y_val, y_pred)\n",
    "confusion_matrix_val_JJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_val_JJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_training, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for reference\n",
    "#def final_cv_array(top_n_words_array, df, column_name = 'content_clean'):\n",
    "NN_array_training = final_cv_array(top_50_frequent_NN_words_training, df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_features and y_train are same as previous 'JJ'\n",
    "X_train_NN = np.concatenate((X_train_features, NN_array_training), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_NN, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = logreg.predict(X_train_NN)\n",
    "confusion_matrix_training_NN = confusion_matrix(y_train, y_pred)\n",
    "confusion_matrix_training_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_training_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_validation, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_array_validation = final_cv_array(top_50_frequent_NN_words_training, df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val_features and y_val are same as previous 'JJ'\n",
    "X_val_NN = np.concatenate((X_val_features, NN_array_validation), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_val_NN)\n",
    "confusion_matrix_val_NN = confusion_matrix(y_val, y_pred)\n",
    "confusion_matrix_val_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_val_NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_training, VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VB_array_training = final_cv_array(top_50_frequent_VB_words_training, df_training)\n",
    "\n",
    "#X_train_features and y_train are same as previous 'JJ'\n",
    "X_train_VB = np.concatenate((X_train_features, VB_array_training), axis = 1)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_VB, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = logreg.predict(X_train_VB)\n",
    "confusion_matrix_training_VB = confusion_matrix(y_train, y_pred)\n",
    "confusion_matrix_training_VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_training_VB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_validation, VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VB_array_validation = final_cv_array(top_50_frequent_VB_words_training, df_validation)\n",
    "\n",
    "#X_val_features and y_val are same as previous 'JJ'\n",
    "X_val_VB = np.concatenate((X_val_features, VB_array_validation), axis = 1)\n",
    "\n",
    "y_pred = logreg.predict(X_val_VB)\n",
    "confusion_matrix_val_VB = confusion_matrix(y_val, y_pred)\n",
    "confusion_matrix_val_VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_val_VB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_training, CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_array_training = final_cv_array(top_50_frequent_CD_words_training, df_training)\n",
    "\n",
    "#X_train_features and y_train are same as previous 'JJ'\n",
    "X_train_CD = np.concatenate((X_train_features, CD_array_training), axis = 1)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_CD, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_train_CD)\n",
    "confusion_matrix_training_CD = confusion_matrix(y_train, y_pred)\n",
    "confusion_matrix_training_CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_training_CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_validation, CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD_array_validation = final_cv_array(top_50_frequent_CD_words_training, df_validation)\n",
    "\n",
    "#X_val_features and y_val are same as previous 'JJ'\n",
    "X_val_CD = np.concatenate((X_val_features, CD_array_validation), axis = 1)\n",
    "\n",
    "y_pred = logreg.predict(X_val_CD)\n",
    "confusion_matrix_val_CD = confusion_matrix(y_val, y_pred)\n",
    "confusion_matrix_val_CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_val_CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN and VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_VB_array_training = final_cv_array(top_50_frequent_NN_VB_words_training, df_training)\n",
    "\n",
    "#X_train_features and y_train are same as previous 'JJ'\n",
    "X_train_NN_VB = np.concatenate((X_train_features, NN_VB_array_training), axis = 1)\n",
    "\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_NN_VB, y_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = logreg.predict(X_train_NN_VB)\n",
    "confusion_matrix_training_NN_VB = confusion_matrix(y_train, y_pred)\n",
    "confusion_matrix_training_NN_VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_training_NN_VB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_VB_array_validation = final_cv_array(top_50_frequent_NN_VB_words_training, df_validation)\n",
    "\n",
    "#X_val_features and y_val are same as previous 'JJ'\n",
    "X_val_NN_VB = np.concatenate((X_val_features, NN_VB_array_validation), axis = 1)\n",
    "\n",
    "y_pred = logreg.predict(X_val_NN_VB)\n",
    "confusion_matrix_val_NN_VB = confusion_matrix(y_val, y_pred)\n",
    "confusion_matrix_val_NN_VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(confusion_matrix_val_NN_VB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "logreg = LogisticRegression(max_iter=300)\n",
    "print(\"JJ Train Score \" + str(cross_val_score(logreg, X_train, y_train, cv=10).mean()))\n",
    "print(\"NN Train Score \" + str(cross_val_score(logreg, X_train_NN, y_train, cv=10).mean()))\n",
    "print(\"VB Train Score \" + str(cross_val_score(logreg, X_train_VB, y_train, cv=10).mean()))\n",
    "print(\"CD Train Score \" + str(cross_val_score(logreg, X_train_CD, y_train, cv=10).mean()))\n",
    "print(\"NN and VB Train Score \" + str(cross_val_score(logreg, X_train_NN_VB, y_train, cv=10).mean()))\n",
    "#remember to record in spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_NN,y_train)\n",
    "y_pred_NN=clf.predict(X_val_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_val, y_pred_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_VB,y_train)\n",
    "y_pred_VB=clf.predict(X_val_VB)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_val, y_pred_VB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train_CD,y_train)\n",
    "y_pred_CD=clf.predict(X_val_CD)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_val, y_pred_CD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pseudocode\n",
    "#inputs: dataframe, n word length, POS\n",
    "#create a dataframe with column names \"num_chars\", \"POS\", \"top num_words, \"validation accuracy\"\n",
    "#for num_chars in range(3,10):\n",
    "#   for POS in POS_list:\n",
    "#      for num_words in top_n_words:\n",
    "#         Count Vectorizer stuff\n",
    "#         Run model, get accuracy\n",
    "#output: list of accuracies based on each POS and word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs: dataframe, n word length, POS\n",
    "#chagne functions to include verbs like \"get_word\"\n",
    "model_accuracy = pd.DataFrame(columns=['num_chars','POS','top_num_words', 'validation_accuracy']) #df with columns\n",
    "POS_list = ['JJ', 'NN', 'VB', 'CD'] #list of POS\n",
    "\n",
    "#PUT COMMENTS IN HEREEEE!!!\n",
    "for num_chars in range(1,8): #filters out words less than num_chars \n",
    "    for POS in POS_list:\n",
    "        for num_words in [10, 50, 100, 150, 200, 1000]:\n",
    "            my_vocab = list_of_vocab(df_training, POS, num_chars) #creates list of vocab to be put in CV based on df_training dataset\n",
    "            n_words_array = top_n_words_array(my_vocab, df_training, 'content_clean', num_words)\n",
    "            cv_array = final_cv_array(n_words_array, df_training,)\n",
    "            X_train_features = df_training[['sentiment', 'adj_proportion', 'noun_proportion', 'verb_proportion', 'c_digit_proportion']].to_numpy()\n",
    "            y_train = df_training['leaning'].apply(lambda x: int(x))\n",
    "            X_train = np.concatenate((X_train_features, cv_array), axis = 1)\n",
    "            logreg = LogisticRegression()\n",
    "            logreg.fit(X_train, y_train) #fits model (with df_training data)\n",
    "            X_val_features = df_validation[['sentiment', 'adj_proportion', 'noun_proportion', 'verb_proportion', 'c_digit_proportion']].to_numpy()\n",
    "            y_train_val = df_validation['leaning'].apply(lambda x: int(x))\n",
    "            cv_array_val = final_cv_array(n_words_array, df_validation,)\n",
    "            X_val = np.concatenate((X_val_features, cv_array_val), axis = 1)\n",
    "            y_pred_val = logreg.predict(X_val) #uses logreg on df_validation this time\n",
    "            confusion_matrix_val = confusion_matrix(y_val, y_pred_val)\n",
    "            my_overall_accuracy = overall_accuracy(confusion_matrix_val)\n",
    "            model_accuracy = model_accuracy.append({'num_chars': num_chars,\n",
    "                                  'POS': POS,\n",
    "                                  'top_num_words': num_words,                 \n",
    "                                  'validation_accuracy': my_overall_accuracy}, ignore_index = True)  #adds row to model_accuracy each time   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_accuracy.groupby(['num_chars']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy.groupby(['POS']).mean().reset_index()\n",
    "#cardinal digits had a significantly lower total accuracy in comparison to the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy.groupby(['top_num_words']).mean().reset_index()\n",
    "#the more words used, the more accurate it seems to be until around 150 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy.sort_values(['validation_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each_POS in ['JJ', 'NN', 'VB', 'CD']:\n",
    "    plt.plot('num_chars', 'validation_accuracy', data=model_accuracy[model_accuracy.POS == each_POS].groupby(['num_chars']).mean().reset_index(), label=each_POS)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for each_num_words in [10,50,100,150,1000]:\n",
    "    plt.plot('num_chars', 'validation_accuracy', data=model_accuracy[model_accuracy.top_num_words == each_num_words].groupby(['num_chars']).mean().reset_index(), label=each_num_words)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
